%-------------------------------------------------------------------------------
\section{Methodology}
%-------------------------------------------------------------------------------
\begin{figure}[h]
\begin{center}
\includegraphics[width=0.4\textwidth, scale=0.25]{system_model.pdf}
\caption{System Model} 
\label{System_model}
\end{center}
\end{figure}

There are many possible mappings to obtain embeddings from traces. The choice of embedding is typically driven by the downstream task to be performed. Our approach is to learn embeddings that encode structural information about the trace automatically from example traces. In particular, we compute our graph embedding as a composite of embeddings of nodes in the graph, as this is the approach commonly taken in previous work~\cite{corr_2017_abs-1709-05584}. Of the various aggregator functions available such as summation, graph-coarsening approaches and graph neural networks, we chose summation for its simplicity and scalability. For precisely these reasons, it has been used in prior work~\cite{DBLP:journals/corr/DuvenaudMAGHAA15, DBLP:journals/corr/DaiDS16} to obtain subgraph embeddings. Intuitively, the resultant graph embedding is a linear combination of the component node embeddings. 

Grover et al.~\cite{corr18_GroverL16} describe node2vec, which obtains node embeddings for graphs analogous to word embeddings obtained for documents. Graph nodes are considered words and random walks in the graph generate sentences for us to train on. As a result, the node embeddings encode information about its structural neighborhood. We use node2vec to obtain component embeddings.  

For a graph G, a naive approach might be to encode the trace as a node array, where the number of occurrences of an event is indicated by an integer n. This baseline embedding R is a vector of dimension D, where D is the the number of unique events. 
Let $E_{i},\; i \in {0..D-1} $ represent the set of unique events. Further, let $Map(E_{i}),\; i \in {0..D-1} $ be the mapping from events to array indices, such that $ Map(E_{i}) \neq Map(E_{j}),\;  i \neq j,\; 0 \leq i,j \leq D-1 $ We now have: \newline
\begin{math}
m_{i} =\; Map(E_{i})\; \forall i \in {0..D-1} \\
R[m_{i}] = n_{i},\;  
\end{math}
where $n_{i}$ is the number of occurrences of the event $ E(m_{i})$\\

The distance measure used to compute distance between embeddings is the Euclidean distance. In the next section, we describe how our approach compares vis-a-vis the baseline as well as initial results for each of the tasks in the figure.