%-------------------------------------------------------------------------------
\section{Related Work}
%-------------------------------------------------------------------------------

Post factum analysis of traces is important because they contain useful information like causality of events, resource consumption, errors, performance data and labeled metadata about components involved in the execution.
In large systems the size of the traces makes it impractical for a developer to do any manual analysis. This motivates the need for automated analysis of large corpus of traces to derive useful insights.
There are many goals for analysis for traces, including finding performance bottlenecks, system recovery \& repair, debugging, resource usage, service degradation analysis.
Many of these use cases use some form of classification or clustering. We describe some of the prior work in this area and highlight the drawbacks and explain how our approach overcomes some of the hurdles.

Magpie \cite{Barham:2003:MOM:1251054.1251069} relies on structured events produced logged by ETW (event tracing for windows) in order to avoid the need to have unique identifiers for every request.
Magpie correlates events generated by the application, middleware and operating system using temporal joins to infer causal relationships. 
Magpie uses a comparison based on a simple string-edit-distance metric on flattened execution graphs as a basis for execution clustering.
Even though the technique shows promising results even for classification of requests with differences in internal concurrency structures, the authors acknowledge the need for graph and tree edit distances.

Pinpoint \cite{Chen:2004:PFE:1251175.1251198} collects execution traces as a series of paths through the system. The authors diagnose anomalies by generating a probabilistic context-free grammar (PCFG) from the paths. 
They perform anomaly detection at runtime, whereby new paths are compared to the generated model to determine the probability with which it would be produced from the grammar. 
Anomaly detection worked well in experiments, but the authors note that there are a number of realistic scenarios where it would
not work well: features must be represented in the training set for them to be considered at runtime; changes
such as software upgrade require the model to be retrained; and the learned model represents a superset of observed paths.

Spectroscope \cite{Sambasivan:2011:DPC:1972457.1972463} collects execution traces represented as process invocation trees, and diagnoses performance
changes by comparing sets of before and after traces. Spectroscope assumes that a similar workload was run before and after the performance change, and that the performance change manifests as a change in distribution
over the request structures and/or request timings. To diagnose a change, spectroscope compares the distributions of service completion times for graphs that are topologically identical, and compares structural differences
between executions using string-edit-distance.

Mann et al. \cite{Mann:2011:MPE:2170444.2170464} collect execution traces from datacenter services, and model the latency of a service given the child services invoked.
This work focuses on learning join-point locations by comparing large volumes of Dapper traces.
The execution graphs recorded do not fully capture the causal dependencies internal to a service, so one component of the work is to deduce those causal dependencies from a collection of training examples.
The training examples are then clustered if they have identical execution graphs. 
At runtime, a cluster is selected by comparing its service timings with the cluster centroids, and selecting the nearest-neighbour. 
A prediction for the executionâ€™s overall runtime is then given based on the other executions in the selected cluster.

Mann et al. and Spectroscope do clustering, but only for graphs that are isomorphic.
For graphs with different topology, Spectroscope and Magpie use string-edit distance on canonicalized and linearized graphs as a metric,
for proper clustering in the latter, and for finding similar clusters in the former.
Magpie and Pinpoint cluster similar behavior and suggest outliers as possible indicators of bugs.

Pip \cite{Reynolds2006PipDT} takes a different approach to solving these problems. 
Unlike Magpie, Pip does not rely on any statistical inference and hence obviates the need to collect a large corpus of traces.
Pip treats the systems as \textit{grey-box} and finds structural and performance bugs in distributed systems by comparing actual system behavior to expected system behavior.
Pip asks the developers to specify the expectations of the program using a declarative specification language, while capturing the actual system behavior using instrumentation, annotations or sniffing.
Pip automatically checks actual behavior against expected behavior and helps programmers visualize the result to discover the causes of any unexpected behavior.
However, pip cannot be used for analysis of running programs since it waits for the program to terminate before reconciling traces. 
This makes pip unsuitable for use in online services. 

Las-Casas et al. \cite{Las-Casas:2018:WSE:3267809.3267841} have described initial work on using incremental heirarchical clusterting to solve the problem of representative trace sampling.
The technique uses the PERCH \cite{Kobren:2017:HAE:3097983.3098079} algorithm for incremental heirarchical clustering and can be used for both offline and online sampling.
The technique offers considerable improvements over random sampling for capturing anomalous traces as well as reconstruction of traces from a sample.
For reconstruction of traces they also rely on euclidean edit distances, which we seek to avoid by using representational learning.

\kam{
The other line of related work which I think will be useful to mention is that there has been some work in the systems community around using NLP techniques for systems problems. 
I attended a talk in CIDR where word2vec was utilized for database workload management and have included it in the references. We might want to look around if there is some other work that I've missed.}